package com.zongteng.ztetl.util.stream

import java.util

import com.zongteng.ztetl.api.{HBaseConfig, SparkKafkaCon}
import com.zongteng.ztetl.common.kafka.offset.KafkaOffsetManager
import com.zongteng.ztetl.common.zookeeper.ZkConf
import com.zongteng.ztetl.entity.common.InsertModel
import com.zongteng.ztetl.util.PropertyFile
import kafka.api.OffsetRequest
import kafka.utils.ZkUtils
import org.I0Itec.zkclient.{ZkClient, ZkConnection}
import org.apache.commons.lang3.StringUtils
import org.apache.hadoop.hbase.TableName
import org.apache.hadoop.hbase.client.{Connection, Put, Table}
import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.kafka.common.TopicPartition
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.{Accumulator, SparkConf}
import org.apache.spark.rdd.RDD
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.dstream.InputDStream

object SparkStreamCodeUtilGcTcms {

  /**
    * 生成实时导入ods层代码的工具类
    *
    * @param appName        作业名称
    * @param interval       消费时间间隔
    * @param topics         kafka中topic名称
    * @param groupId        消费组
    * @param mysql_database mysql数据库名称
    * @param datasource_name 数据来源简称（例如：gc_oms）
    */
  def getRunCode(appName: String, interval: Int, topics: Array[String], groupId: String, mysql_database: String, datasource_name: String) = {

    // spark配置参数
    val sparkConf = new SparkConf()

    // 反压参数
    sparkConf.set("spark.streaming.backpressure.enabled", "true") // 激活削峰（反压）功能
    sparkConf.set("spark.streaming.backpressure.initialRate","100")// 第一次读取的最大数据值
    sparkConf.set("spark.streaming.receiver.maxRate", "200") // 每个receiver 每秒最大可以接收的记录的数据
    sparkConf.set("spark.streaming.kafka.maxRatePerPartition", "200") // 限制每次作业中每个Kafka分区最多读取的记录条数

//    sparkConf.set("spark.streaming.stopGracefullyOnShutdown","true")// 优雅的关闭
//    sparkConf.set("spark.streaming.backpressure.initialRate","100")// 第一次读取的最大数据值

    // kafka配置参数
    var kafkaParams: Map[String, Object] = Map[String, Object](
      "bootstrap.servers" -> PropertyFile.getProperty("kafka_sesrver"),
      "key.deserializer" -> classOf[StringDeserializer],
      "value.deserializer" -> classOf[StringDeserializer],
      "group.id" -> groupId,
      //"auto.offset.reset" -> "latest",
      "auto.offset.reset" -> "earliest",
      "enable.auto.commit" -> (false: java.lang.Boolean)
    )

    // zookeeper
    val tuple2: (ZkClient, ZkConnection) = ZkConf.getZkClientAndConnection(30000, 30000)
    val zkClient: ZkClient = tuple2._1
    val zkConnection: ZkConnection = tuple2._2
    val zkutils: ZkUtils = new ZkUtils(zkClient, zkConnection, true)

    val zkOffsetPath = "/kafka_offset_manager/consumers/consumer_group/" + groupId

    // 读取偏移量信息
    val partitionToLongOption: Option[Map[TopicPartition, Long]] = KafkaOffsetManager.readOffset(zkutils, zkOffsetPath, topics)

    val streamAndInputDStream: (StreamingContext, InputDStream[ConsumerRecord[String, String]]) = partitionToLongOption match {
      case Some(partitionToLong) => SparkKafkaCon.getConnectKafka(appName, interval, sparkConf, topics, kafkaParams) // 消费者已经启动过，获取最后一次提交的偏移量
      case None => {
        kafkaParams += ("auto.offset.reset" -> "earliest")
        SparkKafkaCon.getConnectKafka(appName, interval, sparkConf, topics, kafkaParams)
      } // 消费者第一次启动
    }

    val streamingContext: StreamingContext = streamAndInputDStream._1
    val kafkaInputDstream: InputDStream[ConsumerRecord[String, String]] = streamAndInputDStream._2

    println("streamingContext == " + streamingContext)
    println("kafkaInputDstream == " + kafkaInputDstream)

    try {

      kafkaInputDstream.foreachRDD((rdd: RDD[ConsumerRecord[String, String]]) => {

        println("============" + rdd.isEmpty())

        if (!rdd.isEmpty()) {

          // filter作用：只消费指定数据库指定表的数据
          println("数据量 " + rdd.count())

          var count = 0;
          rdd.filter((e: ConsumerRecord[String, String]) => checkJsonStr(mysql_database, e.value(), datasource_name)).foreach((e: ConsumerRecord[String, String]) => {
            count += 1
//            // hbase连接（必须在这里定义，否则会遇到序列化的问题）
//            val connection: Connection = HBaseConfig.getConnection()
//
//           // println(e.value())
//
//            // 获取对应的row数据
//            val model: InsertModel = SparkJsonUtil.getInsertObject(e.value())
//            val puts: util.ArrayList[Put] = SparkJsonUtil.getHBasePut(model)
//
//            // 数据添加到hbase中
//            // val table: Table = connection.getTable(TableName.valueOf(ods_table))
//
//            val ods_table = datasource_name + "_" + model.table
//            val table: Table = connection.getTable(TableName.valueOf(ods_table))
//
//            for (i <- 0 until puts.size) {
//              val put: Put = puts.get(i)
//              table.put(put)
//              println( ods_table + " 添加成功 " + put)
//            }
//            table.close()
            println("累加器 == " + count + " == " + e.value())
          })

          //throw new Exception("哈哈哈哈哈哈")

          // 提交偏移量
          KafkaOffsetManager.savaOffsets(zkutils, zkClient, zkOffsetPath, rdd)
        }
      })

      //Log.end(task)
    } catch {
      case e: Exception => e.getMessage
        kafkaInputDstream.stop()
      //Log.end(task)
    }

    streamingContext.start()
    streamingContext.awaitTermination()
    kafkaInputDstream.stop()
  }

  def checkJsonStr(mysql_database: String, jsonStr: String, datasource_name: String) = {

//    val allow_mysql_table: Array[String] = AllowMysqlTable.table
//    var result = false
//
//    // println(mysql_database + "  ===  " + jsonStr)
//    if (StringUtils.isNotBlank(jsonStr)) {
//      val model: InsertModel = SparkJsonUtil.getInsertObject(jsonStr)
//      result = mysql_database.equalsIgnoreCase(model.database) &&
//        allow_mysql_table.contains(datasource_name + "_" + model.table)
//    }
//
//    result

    true
  }


}
